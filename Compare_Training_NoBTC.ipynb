{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Compare BTC with No BTC price input data\n",
    "#\n",
    "# MLND Capstone > Crypto Ethereum Future Price RNN Classifier \n",
    "# Input - Ethereum minute price data only, no BTC data used\n",
    "# Output - A Prediction very close to 0.5\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Check installed python and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.2 (default, Nov 12 2018, 13:43:14) \\n[GCC 5.4.0 20160609]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check python version\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "# check tensorflow, keras version and GPU\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print (tf.VERSION)\n",
    "\n",
    "# Check Keras Version\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "# Check for a GPU\n",
    "# print (tf.test.gpu_device_name())\n",
    "# notice - this code line uses GPU VRam\n",
    "# if executed and GPU VRam is low, then restart notebook kernel to free GPU VRam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Load Dataset from the 1 csv file for ethereum, No bitcoin data to see the difference in training accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73f0d4c2c6324b8a94a95db7796ac069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# load Dataset from the two csv files\n",
    "#   gemini_BTCUSD_2018_1min.csv\n",
    "#   gemini_ETHUSD_2018_1min.csv\n",
    "################################################\n",
    "\n",
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime as time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import time\n",
    "\n",
    "data_folder = './data/minute/'\n",
    "\n",
    "# data_files = ['gemini_BTCUSD_2018_1min.csv', 'gemini_ETHUSD_2018_1min.csv']\n",
    "data_files = ['gemini_ETHUSD_2018_1min.csv']\n",
    "\n",
    "def date_format(date_str):\n",
    "    return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(date_str))\n",
    "    \n",
    "def build_dataset():\n",
    "    \n",
    "    df_main = pd.DataFrame() \n",
    "    \n",
    "    for file in tqdm_notebook(data_files):\n",
    "        if file.endswith('.csv'):\n",
    "            \n",
    "            # get info from filename\n",
    "            exchange, pair, year, _ = file.replace('.csv', '').split('_')\n",
    "        \n",
    "            # load data .csv file\n",
    "            file_path = os.path.join(data_folder, file)\n",
    "            df = pd.read_csv(file_path, parse_dates=True, skiprows=2, names=['Date', 'Symbol', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "            \n",
    "            # date already in correct format yyyy-mm-dd hh:mm:ss\n",
    "            # no need to reformat\n",
    "            # df['Date'] = df['Date'].apply(lambda x: date_format(x))\n",
    "            \n",
    "            # set Date as index column\n",
    "            df.set_index('Date', inplace=True)\n",
    "\n",
    "            # rename columns\n",
    "            rename_cols_from = ['Close', 'Volume']\n",
    "            rename_cols_to = ['{}_{}_Close'.format(exchange, pair), '{}_{}_Volume'.format(exchange, pair)]\n",
    "            df.rename(columns={rename_cols_from[0]: rename_cols_to[0], rename_cols_from[1]: rename_cols_to[1]}, inplace=True)\n",
    "\n",
    "            # reduce to only columns you want to keep\n",
    "            df = df[[rename_cols_to[0], rename_cols_to[1]]]\n",
    "                        \n",
    "            # join all csv data into one dataframe\n",
    "            if len(df_main) == 0:  \n",
    "                df_main = df\n",
    "            else:\n",
    "                df_main = df_main.join(df)\n",
    "        \n",
    "            \n",
    "    # fill any missing data\n",
    "    df_main.fillna(method=\"ffill\", inplace=True)\n",
    "\n",
    "    # delete any na\n",
    "    df_main.dropna(inplace=True)\n",
    "    \n",
    "    return df_main\n",
    "            \n",
    "df_main = build_dataset()          \n",
    "        \n",
    "# order time sequence ASC\n",
    "# 2018-01-01 to 2018-12-31\n",
    "df_main.sort_index(inplace=True)\n",
    "\n",
    "# remove any bad data\n",
    "# the following index is isolated and not useful to predict sequence future data\n",
    "# index=0, index value=2017-09-22 19:00:00\n",
    "df_main.drop(df_main.index[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Loaded data verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gemini_ETHUSD_Close</th>\n",
       "      <th>gemini_ETHUSD_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:01:00</th>\n",
       "      <td>737.98</td>\n",
       "      <td>2.410785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:02:00</th>\n",
       "      <td>736.03</td>\n",
       "      <td>1.613000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:03:00</th>\n",
       "      <td>738.29</td>\n",
       "      <td>1.135121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:04:00</th>\n",
       "      <td>738.29</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:05:00</th>\n",
       "      <td>735.00</td>\n",
       "      <td>66.676885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     gemini_ETHUSD_Close  gemini_ETHUSD_Volume\n",
       "Date                                                          \n",
       "2018-01-01 00:01:00               737.98              2.410785\n",
       "2018-01-01 00:02:00               736.03              1.613000\n",
       "2018-01-01 00:03:00               738.29              1.135121\n",
       "2018-01-01 00:04:00               738.29              0.000000\n",
       "2018-01-01 00:05:00               735.00             66.676885"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Exploration - preview first 5 records\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gemini_ETHUSD_Close</th>\n",
       "      <th>gemini_ETHUSD_Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:55:00</th>\n",
       "      <td>130.8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:56:00</th>\n",
       "      <td>130.0</td>\n",
       "      <td>79.267075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:57:00</th>\n",
       "      <td>130.0</td>\n",
       "      <td>44.386537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:58:00</th>\n",
       "      <td>130.0</td>\n",
       "      <td>7.966173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31 23:59:00</th>\n",
       "      <td>130.8</td>\n",
       "      <td>8.688215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     gemini_ETHUSD_Close  gemini_ETHUSD_Volume\n",
       "Date                                                          \n",
       "2018-12-31 23:55:00                130.8              0.000000\n",
       "2018-12-31 23:56:00                130.0             79.267075\n",
       "2018-12-31 23:57:00                130.0             44.386537\n",
       "2018-12-31 23:58:00                130.0              7.966173\n",
       "2018-12-31 23:59:00                130.8              8.688215"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Exploration - preview last 5 records\n",
    "df_main.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks correct and well formatted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The list of columns in joined dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini_ETHUSD_Close\n",
      "gemini_ETHUSD_Volume\n"
     ]
    }
   ],
   "source": [
    "for c in df_main.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Technical Analysis    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip as not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Implemenation\n",
    "#### 4.1 Define Varibles for Lookback history sequence and Target column     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model learn by lookback history\n",
    "# the longer the lookback period the better, but more data will take longer to train\n",
    "LOOKBACK_HISTORY_SEQ_LEN = 100 # look back last 100 minutes \n",
    "\n",
    "# How far we wish to predict the future?\n",
    "PREDICT_FUTURE_SEQ_LEN = 5 # predict 5 minute in the future\n",
    "\n",
    "# We are predicting the future price of Ethereum in exchange Gemini\n",
    "PREDICT_COLUMN = 'gemini_ETHUSD_Close'\n",
    "\n",
    "# Could switch to bitcoin, or any other altcoin for future development\n",
    "# PREDICT_COLUMN = 'gemini_BTCUSD_Close' \n",
    "\n",
    "# Create a new column to hold the future price of Ethereum\n",
    "# this allow the calculation of price raised or falled.\n",
    "PREDICT_COLUMN_FUTUTE = '{}_Future'.format(PREDICT_COLUMN)\n",
    "\n",
    "# The target label column \n",
    "# holding binary values indicate the future price is either: \n",
    "# If future price Raised, Action=Buy, Stored Value=1\n",
    "# If future price Falled, Action=Sell, Stored Value=0\n",
    "TARGET_LABEL = 'Target_Action'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date\n",
       "2018-01-01 00:01:00    737.98\n",
       "2018-01-01 00:02:00    736.03\n",
       "2018-01-01 00:03:00    738.29\n",
       "2018-01-01 00:04:00    738.29\n",
       "2018-01-01 00:05:00    735.00\n",
       "2018-01-01 00:06:00    736.00\n",
       "2018-01-01 00:07:00    736.20\n",
       "2018-01-01 00:08:00    736.20\n",
       "2018-01-01 00:09:00    736.20\n",
       "2018-01-01 00:10:00    736.31\n",
       "2018-01-01 00:11:00    736.21\n",
       "2018-01-01 00:12:00    736.21\n",
       "2018-01-01 00:13:00    736.20\n",
       "2018-01-01 00:14:00    736.20\n",
       "2018-01-01 00:15:00    735.00\n",
       "2018-01-01 00:16:00    733.01\n",
       "2018-01-01 00:17:00    733.01\n",
       "2018-01-01 00:18:00    733.01\n",
       "2018-01-01 00:19:00    732.99\n",
       "2018-01-01 00:20:00    732.99\n",
       "2018-01-01 00:21:00    732.99\n",
       "2018-01-01 00:22:00    732.99\n",
       "2018-01-01 00:23:00    732.99\n",
       "2018-01-01 00:24:00    732.99\n",
       "2018-01-01 00:25:00    732.99\n",
       "2018-01-01 00:26:00    731.54\n",
       "2018-01-01 00:27:00    730.61\n",
       "2018-01-01 00:28:00    731.01\n",
       "2018-01-01 00:29:00    733.00\n",
       "2018-01-01 00:30:00    733.00\n",
       "                        ...  \n",
       "2018-12-31 23:30:00    130.91\n",
       "2018-12-31 23:31:00    130.91\n",
       "2018-12-31 23:32:00    130.97\n",
       "2018-12-31 23:33:00    130.86\n",
       "2018-12-31 23:34:00    130.50\n",
       "2018-12-31 23:35:00    130.00\n",
       "2018-12-31 23:36:00    130.00\n",
       "2018-12-31 23:37:00    130.00\n",
       "2018-12-31 23:38:00    131.12\n",
       "2018-12-31 23:39:00    130.32\n",
       "2018-12-31 23:40:00    130.00\n",
       "2018-12-31 23:41:00    130.00\n",
       "2018-12-31 23:42:00    130.00\n",
       "2018-12-31 23:43:00    130.00\n",
       "2018-12-31 23:44:00    130.00\n",
       "2018-12-31 23:45:00    130.80\n",
       "2018-12-31 23:46:00    131.03\n",
       "2018-12-31 23:47:00    130.80\n",
       "2018-12-31 23:48:00    130.80\n",
       "2018-12-31 23:49:00    130.80\n",
       "2018-12-31 23:50:00    130.80\n",
       "2018-12-31 23:51:00    130.80\n",
       "2018-12-31 23:52:00    130.80\n",
       "2018-12-31 23:53:00    130.80\n",
       "2018-12-31 23:54:00    130.80\n",
       "2018-12-31 23:55:00    130.80\n",
       "2018-12-31 23:56:00    130.00\n",
       "2018-12-31 23:57:00    130.00\n",
       "2018-12-31 23:58:00    130.00\n",
       "2018-12-31 23:59:00    130.80\n",
       "Name: gemini_ETHUSD_Close, Length: 490111, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Exploration - preview column data, and check no NaN\n",
    "df_main[PREDICT_COLUMN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Binary Classification\n",
    "the binary values are Buy=1 or Sell=0<br/>\n",
    "The function binary_classify() is map to the dataset to generate values in target column<br/>\n",
    "if future price raised, then buy representation value 1 is assigned<br/>\n",
    "if future price falled, then sell representation value 0 is assigned<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric Representation of Buy and Sell\n",
    "SELL = 0\n",
    "BUY = 1\n",
    "\n",
    "# function to calculate values to assign target value in each data point.\n",
    "def binary_classify(current_price, future_price):\n",
    "    if(float(future_price) < float(current_price)):\n",
    "        return SELL\n",
    "    else:\n",
    "        return BUY\n",
    "    \n",
    "def create_target_label_column(df):\n",
    "    # create a new column for future price (the future price is already in dataset) \n",
    "    # shift target column data upwards to create the future data\n",
    "    # there is now a current price and a future price for each data row\n",
    "    # the current price and future price is then pass to binary_classify() to calculate target label buy/sell\n",
    "    df[PREDICT_COLUMN_FUTUTE] = df[PREDICT_COLUMN].shift(-PREDICT_FUTURE_SEQ_LEN)\n",
    "\n",
    "    # create a new column for target action buy or sell\n",
    "    df[TARGET_LABEL] = list(map(binary_classify, df[PREDICT_COLUMN], df[PREDICT_COLUMN_FUTUTE])) \n",
    "    \n",
    "    return df\n",
    "\n",
    "df_main = create_target_label_column(df_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gemini_ETHUSD_Close</th>\n",
       "      <th>gemini_ETHUSD_Close_Future</th>\n",
       "      <th>Target_Action</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:01:00</th>\n",
       "      <td>737.98</td>\n",
       "      <td>736.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:02:00</th>\n",
       "      <td>736.03</td>\n",
       "      <td>736.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:03:00</th>\n",
       "      <td>738.29</td>\n",
       "      <td>736.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:04:00</th>\n",
       "      <td>738.29</td>\n",
       "      <td>736.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:05:00</th>\n",
       "      <td>735.00</td>\n",
       "      <td>736.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:06:00</th>\n",
       "      <td>736.00</td>\n",
       "      <td>736.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:07:00</th>\n",
       "      <td>736.20</td>\n",
       "      <td>736.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:08:00</th>\n",
       "      <td>736.20</td>\n",
       "      <td>736.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:09:00</th>\n",
       "      <td>736.20</td>\n",
       "      <td>736.20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:10:00</th>\n",
       "      <td>736.31</td>\n",
       "      <td>735.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:11:00</th>\n",
       "      <td>736.21</td>\n",
       "      <td>733.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:12:00</th>\n",
       "      <td>736.21</td>\n",
       "      <td>733.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:13:00</th>\n",
       "      <td>736.20</td>\n",
       "      <td>733.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:14:00</th>\n",
       "      <td>736.20</td>\n",
       "      <td>732.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:15:00</th>\n",
       "      <td>735.00</td>\n",
       "      <td>732.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:16:00</th>\n",
       "      <td>733.01</td>\n",
       "      <td>732.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:17:00</th>\n",
       "      <td>733.01</td>\n",
       "      <td>732.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:18:00</th>\n",
       "      <td>733.01</td>\n",
       "      <td>732.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:19:00</th>\n",
       "      <td>732.99</td>\n",
       "      <td>732.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:20:00</th>\n",
       "      <td>732.99</td>\n",
       "      <td>732.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:21:00</th>\n",
       "      <td>732.99</td>\n",
       "      <td>731.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:22:00</th>\n",
       "      <td>732.99</td>\n",
       "      <td>730.61</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:23:00</th>\n",
       "      <td>732.99</td>\n",
       "      <td>731.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:24:00</th>\n",
       "      <td>732.99</td>\n",
       "      <td>733.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:25:00</th>\n",
       "      <td>732.99</td>\n",
       "      <td>733.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:26:00</th>\n",
       "      <td>731.54</td>\n",
       "      <td>733.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:27:00</th>\n",
       "      <td>730.61</td>\n",
       "      <td>732.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:28:00</th>\n",
       "      <td>731.01</td>\n",
       "      <td>732.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:29:00</th>\n",
       "      <td>733.00</td>\n",
       "      <td>731.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 00:30:00</th>\n",
       "      <td>733.00</td>\n",
       "      <td>729.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:11:00</th>\n",
       "      <td>727.00</td>\n",
       "      <td>727.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:12:00</th>\n",
       "      <td>728.10</td>\n",
       "      <td>727.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:13:00</th>\n",
       "      <td>727.01</td>\n",
       "      <td>727.01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:14:00</th>\n",
       "      <td>727.01</td>\n",
       "      <td>728.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:15:00</th>\n",
       "      <td>727.01</td>\n",
       "      <td>728.16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:16:00</th>\n",
       "      <td>727.01</td>\n",
       "      <td>730.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:17:00</th>\n",
       "      <td>727.01</td>\n",
       "      <td>731.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:18:00</th>\n",
       "      <td>727.01</td>\n",
       "      <td>731.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:19:00</th>\n",
       "      <td>728.16</td>\n",
       "      <td>728.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:20:00</th>\n",
       "      <td>728.16</td>\n",
       "      <td>728.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:21:00</th>\n",
       "      <td>730.57</td>\n",
       "      <td>730.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:22:00</th>\n",
       "      <td>731.70</td>\n",
       "      <td>730.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:23:00</th>\n",
       "      <td>731.70</td>\n",
       "      <td>730.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:24:00</th>\n",
       "      <td>728.18</td>\n",
       "      <td>730.98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:25:00</th>\n",
       "      <td>728.18</td>\n",
       "      <td>730.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:26:00</th>\n",
       "      <td>730.98</td>\n",
       "      <td>732.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:27:00</th>\n",
       "      <td>730.98</td>\n",
       "      <td>732.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:28:00</th>\n",
       "      <td>730.98</td>\n",
       "      <td>728.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:29:00</th>\n",
       "      <td>730.98</td>\n",
       "      <td>728.45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:30:00</th>\n",
       "      <td>730.00</td>\n",
       "      <td>728.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:31:00</th>\n",
       "      <td>732.58</td>\n",
       "      <td>728.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:32:00</th>\n",
       "      <td>732.58</td>\n",
       "      <td>728.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:33:00</th>\n",
       "      <td>728.45</td>\n",
       "      <td>728.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:34:00</th>\n",
       "      <td>728.45</td>\n",
       "      <td>728.92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:35:00</th>\n",
       "      <td>728.68</td>\n",
       "      <td>728.70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:36:00</th>\n",
       "      <td>728.68</td>\n",
       "      <td>728.68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:37:00</th>\n",
       "      <td>728.68</td>\n",
       "      <td>726.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:38:00</th>\n",
       "      <td>728.68</td>\n",
       "      <td>727.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:39:00</th>\n",
       "      <td>728.92</td>\n",
       "      <td>727.96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 01:40:00</th>\n",
       "      <td>728.70</td>\n",
       "      <td>726.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     gemini_ETHUSD_Close  gemini_ETHUSD_Close_Future  \\\n",
       "Date                                                                   \n",
       "2018-01-01 00:01:00               737.98                      736.00   \n",
       "2018-01-01 00:02:00               736.03                      736.20   \n",
       "2018-01-01 00:03:00               738.29                      736.20   \n",
       "2018-01-01 00:04:00               738.29                      736.20   \n",
       "2018-01-01 00:05:00               735.00                      736.31   \n",
       "2018-01-01 00:06:00               736.00                      736.21   \n",
       "2018-01-01 00:07:00               736.20                      736.21   \n",
       "2018-01-01 00:08:00               736.20                      736.20   \n",
       "2018-01-01 00:09:00               736.20                      736.20   \n",
       "2018-01-01 00:10:00               736.31                      735.00   \n",
       "2018-01-01 00:11:00               736.21                      733.01   \n",
       "2018-01-01 00:12:00               736.21                      733.01   \n",
       "2018-01-01 00:13:00               736.20                      733.01   \n",
       "2018-01-01 00:14:00               736.20                      732.99   \n",
       "2018-01-01 00:15:00               735.00                      732.99   \n",
       "2018-01-01 00:16:00               733.01                      732.99   \n",
       "2018-01-01 00:17:00               733.01                      732.99   \n",
       "2018-01-01 00:18:00               733.01                      732.99   \n",
       "2018-01-01 00:19:00               732.99                      732.99   \n",
       "2018-01-01 00:20:00               732.99                      732.99   \n",
       "2018-01-01 00:21:00               732.99                      731.54   \n",
       "2018-01-01 00:22:00               732.99                      730.61   \n",
       "2018-01-01 00:23:00               732.99                      731.01   \n",
       "2018-01-01 00:24:00               732.99                      733.00   \n",
       "2018-01-01 00:25:00               732.99                      733.00   \n",
       "2018-01-01 00:26:00               731.54                      733.00   \n",
       "2018-01-01 00:27:00               730.61                      732.99   \n",
       "2018-01-01 00:28:00               731.01                      732.99   \n",
       "2018-01-01 00:29:00               733.00                      731.00   \n",
       "2018-01-01 00:30:00               733.00                      729.00   \n",
       "...                                  ...                         ...   \n",
       "2018-01-01 01:11:00               727.00                      727.01   \n",
       "2018-01-01 01:12:00               728.10                      727.01   \n",
       "2018-01-01 01:13:00               727.01                      727.01   \n",
       "2018-01-01 01:14:00               727.01                      728.16   \n",
       "2018-01-01 01:15:00               727.01                      728.16   \n",
       "2018-01-01 01:16:00               727.01                      730.57   \n",
       "2018-01-01 01:17:00               727.01                      731.70   \n",
       "2018-01-01 01:18:00               727.01                      731.70   \n",
       "2018-01-01 01:19:00               728.16                      728.18   \n",
       "2018-01-01 01:20:00               728.16                      728.18   \n",
       "2018-01-01 01:21:00               730.57                      730.98   \n",
       "2018-01-01 01:22:00               731.70                      730.98   \n",
       "2018-01-01 01:23:00               731.70                      730.98   \n",
       "2018-01-01 01:24:00               728.18                      730.98   \n",
       "2018-01-01 01:25:00               728.18                      730.00   \n",
       "2018-01-01 01:26:00               730.98                      732.58   \n",
       "2018-01-01 01:27:00               730.98                      732.58   \n",
       "2018-01-01 01:28:00               730.98                      728.45   \n",
       "2018-01-01 01:29:00               730.98                      728.45   \n",
       "2018-01-01 01:30:00               730.00                      728.68   \n",
       "2018-01-01 01:31:00               732.58                      728.68   \n",
       "2018-01-01 01:32:00               732.58                      728.68   \n",
       "2018-01-01 01:33:00               728.45                      728.68   \n",
       "2018-01-01 01:34:00               728.45                      728.92   \n",
       "2018-01-01 01:35:00               728.68                      728.70   \n",
       "2018-01-01 01:36:00               728.68                      728.68   \n",
       "2018-01-01 01:37:00               728.68                      726.21   \n",
       "2018-01-01 01:38:00               728.68                      727.95   \n",
       "2018-01-01 01:39:00               728.92                      727.96   \n",
       "2018-01-01 01:40:00               728.70                      726.00   \n",
       "\n",
       "                     Target_Action  \n",
       "Date                                \n",
       "2018-01-01 00:01:00              0  \n",
       "2018-01-01 00:02:00              1  \n",
       "2018-01-01 00:03:00              0  \n",
       "2018-01-01 00:04:00              0  \n",
       "2018-01-01 00:05:00              1  \n",
       "2018-01-01 00:06:00              1  \n",
       "2018-01-01 00:07:00              1  \n",
       "2018-01-01 00:08:00              1  \n",
       "2018-01-01 00:09:00              1  \n",
       "2018-01-01 00:10:00              0  \n",
       "2018-01-01 00:11:00              0  \n",
       "2018-01-01 00:12:00              0  \n",
       "2018-01-01 00:13:00              0  \n",
       "2018-01-01 00:14:00              0  \n",
       "2018-01-01 00:15:00              0  \n",
       "2018-01-01 00:16:00              0  \n",
       "2018-01-01 00:17:00              0  \n",
       "2018-01-01 00:18:00              0  \n",
       "2018-01-01 00:19:00              1  \n",
       "2018-01-01 00:20:00              1  \n",
       "2018-01-01 00:21:00              0  \n",
       "2018-01-01 00:22:00              0  \n",
       "2018-01-01 00:23:00              0  \n",
       "2018-01-01 00:24:00              1  \n",
       "2018-01-01 00:25:00              1  \n",
       "2018-01-01 00:26:00              1  \n",
       "2018-01-01 00:27:00              1  \n",
       "2018-01-01 00:28:00              1  \n",
       "2018-01-01 00:29:00              0  \n",
       "2018-01-01 00:30:00              0  \n",
       "...                            ...  \n",
       "2018-01-01 01:11:00              1  \n",
       "2018-01-01 01:12:00              0  \n",
       "2018-01-01 01:13:00              1  \n",
       "2018-01-01 01:14:00              1  \n",
       "2018-01-01 01:15:00              1  \n",
       "2018-01-01 01:16:00              1  \n",
       "2018-01-01 01:17:00              1  \n",
       "2018-01-01 01:18:00              1  \n",
       "2018-01-01 01:19:00              1  \n",
       "2018-01-01 01:20:00              1  \n",
       "2018-01-01 01:21:00              1  \n",
       "2018-01-01 01:22:00              0  \n",
       "2018-01-01 01:23:00              0  \n",
       "2018-01-01 01:24:00              1  \n",
       "2018-01-01 01:25:00              1  \n",
       "2018-01-01 01:26:00              1  \n",
       "2018-01-01 01:27:00              1  \n",
       "2018-01-01 01:28:00              0  \n",
       "2018-01-01 01:29:00              0  \n",
       "2018-01-01 01:30:00              0  \n",
       "2018-01-01 01:31:00              0  \n",
       "2018-01-01 01:32:00              0  \n",
       "2018-01-01 01:33:00              1  \n",
       "2018-01-01 01:34:00              1  \n",
       "2018-01-01 01:35:00              1  \n",
       "2018-01-01 01:36:00              1  \n",
       "2018-01-01 01:37:00              0  \n",
       "2018-01-01 01:38:00              0  \n",
       "2018-01-01 01:39:00              0  \n",
       "2018-01-01 01:40:00              0  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data column values shifted by -5, and target label is correctly classified\n",
    "df_main[[PREDICT_COLUMN, PREDICT_COLUMN_FUTUTE, TARGET_LABEL]].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Buys: 276151\n",
      "Number of Sells: 213960\n",
      "There are more Buys than Sells.\n"
     ]
    }
   ],
   "source": [
    "number_of_buys = len(df_main[df_main[TARGET_LABEL] == BUY])\n",
    "number_of_sells = len(df_main[df_main[TARGET_LABEL] == SELL])\n",
    "\n",
    "print ('Number of Buys:', number_of_buys)\n",
    "print ('Number of Sells:', number_of_sells)\n",
    "\n",
    "if (number_of_buys > number_of_sells):\n",
    "    print ('There are more Buys than Sells.')\n",
    "elif (number_of_buys < number_of_sells):\n",
    "    print ('There are more Sells than Buys.')\n",
    "else:\n",
    "    print ('There are equal number of Buys and Sells.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data split for Training and Validation\n",
    "training set 95% <br/>\n",
    "validation set 05% <br/>\n",
    "(while data still in correctly ordered in time sequence)<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset split at index time: 2018-12-14 23:35:00\n",
      "The number of data points in training set: 465606\n",
      "The number of data points in validation set: 24505\n"
     ]
    }
   ],
   "source": [
    "def data_split(df):\n",
    "    time_list = sorted(df.index.values)\n",
    "\n",
    "    val_set_percentage = 0.05\n",
    "    chop_at_time_index = -int(val_set_percentage * len(time_list))\n",
    "    chop_at_time = sorted(df_main.index.values)[chop_at_time_index]\n",
    "\n",
    "    df_train = df_main[(df_main.index < chop_at_time)]\n",
    "    df_validation = df_main[(df_main.index >= chop_at_time)]\n",
    "\n",
    "    return df_train, df_validation, chop_at_time\n",
    "    \n",
    "df_train, df_validation, chop_at_time = data_split(df_main)\n",
    "\n",
    "print ('The dataset split at index time:', chop_at_time)\n",
    "print ('The number of data points in training set:', len(df_train))\n",
    "print ('The number of data points in validation set:', len(df_validation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Pre-processing data\n",
    "step 1. drop future column - PREDICT_COLUMN_FUTUTE <br/>\n",
    "step 2. normalize data by pct change <br/>\n",
    "step 3. scale data close to 0 -> 1.0 <br/>\n",
    "step 4. build sequences, each data point get 100 minutes lookback history <br/>\n",
    "step 5. balance data to 50% buys 50% sells <br/>\n",
    "step 6. package features as X, target as y, return <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "def handle_features_zero(df):\n",
    "    for col in df.columns:\n",
    "        \n",
    "        # avoid target as target is 1 or 0\n",
    "        if col != TARGET_LABEL:\n",
    "            \n",
    "            # handling volume values 0.0, \n",
    "            # these 0.0 value will cause pct_change to error by divide by 0\n",
    "            # replace 0.0 with NAN\n",
    "            df[col].replace([0.0], [float('nan')], inplace=True) \n",
    "\n",
    "            # drop any data points have volume value 0.0, as it might be misleading\n",
    "            df.dropna(inplace=True) \n",
    "            \n",
    "    return df\n",
    "    \n",
    "# 2. normalize data - change values to pct change\n",
    "def normalize_to_rate_of_change(df):\n",
    "    for col in df.columns:\n",
    "        if col != TARGET_LABEL:\n",
    "            df[col] = df[col].pct_change()\n",
    "            \n",
    "    return df\n",
    "    \n",
    "# 3. scale data - change values to 0 -> 1.0\n",
    "def scale(df):\n",
    "    for col in df.columns:\n",
    "        if col != TARGET_LABEL:\n",
    "            df[col] = preprocessing.scale(df[col].values)\n",
    "        \n",
    "    # drop any nan as a result of normalize or scale\n",
    "    df = handle_features_nan(df)\n",
    "            \n",
    "    return df\n",
    "    \n",
    "def handle_features_nan(df):\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "    \n",
    "# 4. build sequences of history rows, last 100 minutes\n",
    "def build_history_sequences(df):\n",
    "    \n",
    "    all_sequences = []\n",
    "    next_sequence = deque(maxlen=LOOKBACK_HISTORY_SEQ_LEN)\n",
    "    \n",
    "    for data_row in df.values:\n",
    "        # extract feature columns, ignore target column\n",
    "        features_only_no_target = [n for n in data_row[:-1]]\n",
    "        next_sequence.append(features_only_no_target)\n",
    "    \n",
    "        # once the sequence have enough length, 24 hours, then append ot main list\n",
    "        if len(next_sequence) == LOOKBACK_HISTORY_SEQ_LEN:\n",
    "            all_sequences.append([np.array(next_sequence), data_row[-1]])\n",
    "            \n",
    "    # shuffle to random spread data\n",
    "    random.shuffle(all_sequences)\n",
    "    \n",
    "    return all_sequences\n",
    "    \n",
    "# 5. balance data to 50% buys 50% sells\n",
    "def balance_data(all_sequences):\n",
    "    buy_list = []\n",
    "    sell_list = []\n",
    "    \n",
    "    # first seperate out the buys and sells \n",
    "    for seq, target in all_sequences:\n",
    "        if target == SELL:\n",
    "            sell_list.append([seq, target])\n",
    "        elif target == BUY:\n",
    "            buy_list.append([seq, target])\n",
    "\n",
    "    random.shuffle(buy_list)\n",
    "    random.shuffle(sell_list)\n",
    "    \n",
    "    # work out which havethe smaller len, the crop on that len\n",
    "    lower_len = min(len(buy_list), len(sell_list))\n",
    "\n",
    "    buy_list = buy_list[:lower_len]\n",
    "    sell_list = sell_list[:lower_len]\n",
    "    \n",
    "    # join the list back together and shuffle to spread the buys and sells evenly\n",
    "    balanced_sequences = buy_list + sell_list\n",
    "    random.shuffle(balanced_sequences)\n",
    "    \n",
    "    return balanced_sequences\n",
    "        \n",
    "def package_data(balanced_sequences):\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for seq, target in balanced_sequences:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "        \n",
    "    return np.array(X), y\n",
    "        \n",
    "def data_preprocessing(df):\n",
    "    \n",
    "    # 1. drop future column\n",
    "    df = df.drop(PREDICT_COLUMN_FUTUTE, 1)\n",
    "    \n",
    "    # handle any nan values, by fill forward\n",
    "    df.fillna(method='ffill')\n",
    "        \n",
    "    # remove any volumn with value 0.0\n",
    "    df = handle_features_zero(df)\n",
    "    \n",
    "    # 2. normalize data - change values to pct change\n",
    "    df = normalize_to_rate_of_change(df)\n",
    "                        \n",
    "    # 3. scale data - change values to 0 -> 1.0\n",
    "    df = scale(df)\n",
    "    \n",
    "    # 4. build sequences of history rows, last 100 minutes\n",
    "    all_sequences = build_history_sequences(df)\n",
    "\n",
    "    # 5. balance data to 50% buys 50% sells\n",
    "    balanced_sequences = balance_data(all_sequences)\n",
    "    \n",
    "    # 6. return features as X, target as y\n",
    "    X, y = package_data(balanced_sequences)\n",
    "    \n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code in data_preprocessing creates 4 sets:\n",
    "- X Training\n",
    "- y Training\n",
    "- X Validation\n",
    "- y Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_preprocessing(df_train)\n",
    "X_val, y_val = data_preprocessing(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Save pre-processed data\n",
    "Save the train sets and validation sets for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pre-processed data to pickle - data/preprocessed_no_btc/X_train.pickle\n",
      "Saved pre-processed data to pickle - data/preprocessed_no_btc/y_train.pickle\n",
      "Saved pre-processed data to pickle - data/preprocessed_no_btc/X_val.pickle\n",
      "Saved pre-processed data to pickle - data/preprocessed_no_btc/y_val.pickle\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "save_to_folder = 'data/preprocessed_no_btc/'\n",
    "\n",
    "# create folder data if not exist\n",
    "if not os.path.exists(save_to_folder):\n",
    "    os.makedirs(save_to_folder)\n",
    "\n",
    "save_file = save_to_folder + 'X_train.pickle'\n",
    "pickle.dump(X_train, open(save_file, \"wb\"))\n",
    "print ('Saved pre-processed data to pickle - %s' % save_file)\n",
    "\n",
    "save_file = save_to_folder + 'y_train.pickle'\n",
    "pickle.dump(y_train, open(save_file, \"wb\"))\n",
    "print ('Saved pre-processed data to pickle - %s' % save_file)\n",
    "\n",
    "save_file = save_to_folder + 'X_val.pickle'\n",
    "pickle.dump(X_val, open(save_file, \"wb\"))\n",
    "print ('Saved pre-processed data to pickle - %s' % save_file)\n",
    "\n",
    "save_file = save_to_folder + 'y_val.pickle'\n",
    "pickle.dump(y_val, open(save_file, \"wb\"))\n",
    "print ('Saved pre-processed data to pickle - %s' % save_file)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6 Load pre-proocessed data\n",
    "To Save time, next time in this notebook you dont have to run the previous code cells again, just run code from next cell to use the pre-processed data for RNN model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-import used packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime as time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import time\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-define variables\n",
    "LOOKBACK_HISTORY_SEQ_LEN = 100 \n",
    "PREDICT_FUTURE_SEQ_LEN = 5 \n",
    "PREDICT_COLUMN = 'gemini_ETHUSD_Close'\n",
    "PREDICT_COLUMN_FUTUTE = '{}_Future'.format(PREDICT_COLUMN)\n",
    "TARGET_LABEL = 'Target_Action'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Load the training set and validation set back from previously saved pickle files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pre-processed data - data/preprocessed_no_btc/X_train.pickle\n",
      "Loaded pre-processed data - data/preprocessed_no_btc/y_train.pickle\n",
      "Loaded pre-processed data - data/preprocessed_no_btc/X_val.pickle\n",
      "Loaded pre-processed data - data/preprocessed_no_btc/y_val.pickle\n"
     ]
    }
   ],
   "source": [
    "load_from_folder = 'data/preprocessed_no_btc/'\n",
    "\n",
    "load_file = load_from_folder + 'X_train.pickle'\n",
    "X_train = pickle.load(open(load_file, \"rb\"))\n",
    "print ('Loaded pre-processed data - %s' % load_file)\n",
    "\n",
    "load_file = load_from_folder + 'y_train.pickle'\n",
    "y_train = pickle.load(open(load_file, \"rb\"))\n",
    "print ('Loaded pre-processed data - %s' % load_file)\n",
    "\n",
    "load_file = load_from_folder + 'X_val.pickle'\n",
    "X_val = pickle.load(open(load_file, \"rb\"))\n",
    "print ('Loaded pre-processed data - %s' % load_file)\n",
    "\n",
    "load_file = load_from_folder + 'y_val.pickle'\n",
    "y_val = pickle.load(open(load_file, \"rb\"))\n",
    "print ('Loaded pre-processed data - %s' % load_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of X train data: 277840\n",
      "Size of y train data: 277840\n",
      "Size of X validation data: 12852\n",
      "Size of y validation data: 12852\n"
     ]
    }
   ],
   "source": [
    "# check data len\n",
    "print ('Size of X train data:', len(X_train))\n",
    "print ('Size of y train data:', len(y_train))\n",
    "print ('Size of X validation data:', len(X_val))\n",
    "print ('Size of y validation data:', len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y train buys: 138920\n",
      "y train sells: 138920\n",
      "y validation buys: 6426\n",
      "y validation sells: 6426\n"
     ]
    }
   ],
   "source": [
    "# check target buy=1, sell=0\n",
    "print ('y train buys:', y_train.count(1))\n",
    "print ('y train sells:', y_train.count(0))\n",
    "\n",
    "print ('y validation buys:', y_val.count(1))\n",
    "print ('y validation sells:', y_val.count(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RNN Model for Time Sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 import deep learning and related packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "import time\n",
    "import os\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Tuning Hyper Parameters\n",
    "fine tune model by executing combination of multiple hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models: 1\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# use best model hyper parameters, to speed up training time \n",
    "#\n",
    "# *Model 35/128*\n",
    "# exch-gemini-predict-gemini_ETHUSD_Close-lookback-100-future-5-lstm-2-nodes-128-dense-1-batch-64-dropout-0.2-optimizer-<class 'tensorflow.python.keras.optimizers.Adam'>-lr-0.0001-decay-1e-06\n",
    "# Train on 240418 samples, validate on 11446 samples\n",
    "# val_acc: 0.5744\n",
    "# Test Loss:0.67945953966666\n",
    "# Test Accuracy:0.5744364843925938\n",
    "###    \n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "# model achitecture layers\n",
    "lstm_layers = [2]\n",
    "layer_sizes = [128] \n",
    "dense_layers = [1]\n",
    "batch_sizes = [64]\n",
    "dropouts = [0.2]\n",
    "optimizers = [Adam]\n",
    "learning_rates = [1e-3]\n",
    "decays = [1e-6]\n",
    "loss_functions = ['sparse_categorical_crossentropy']\n",
    "\n",
    "number_of_models = len(lstm_layers) \\\n",
    "                    *len(layer_sizes) \\\n",
    "                    *len(dense_layers) \\\n",
    "                    *len(batch_sizes) \\\n",
    "                    *len(dropouts) \\\n",
    "                    *len(optimizers) \\\n",
    "                    *len(learning_rates) \\\n",
    "                    *len(decays) \\\n",
    "                    *len(loss_functions)\n",
    "\n",
    "print ('Number of models:', number_of_models)\n",
    "\n",
    "# folders for model training and tensorboard\n",
    "logs_to_folder = 'logs_no_btc/'\n",
    "if not os.path.exists(logs_to_folder):\n",
    "    os.makedirs(logs_to_folder)\n",
    "    \n",
    "models_to_folder = 'models_no_btc/'\n",
    "if not os.path.exists(models_to_folder):\n",
    "    os.makedirs(models_to_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Execute RNN model training on each combination of hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*Model 1/1*\n",
      "BTC_VS_NoBTC-exch-gemini-predict-gemini_ETHUSD_Close-lookback-100-future-5-lstm-2-nodes-128-dense-1-batch-64-dropout-0.2-optimizer-<class 'tensorflow.python.keras.optimizers.Adam'>-lr-0.001-decay-1e-06\n",
      "Train on 277840 samples, validate on 12852 samples\n",
      "Epoch 1/10\n",
      "277840/277840 [==============================] - 73s 263us/step - loss: 0.7001 - acc: 0.5102 - val_loss: 0.6930 - val_acc: 0.4987\n",
      "Epoch 2/10\n",
      "277840/277840 [==============================] - 71s 257us/step - loss: 0.6930 - acc: 0.5073 - val_loss: 0.6931 - val_acc: 0.4999\n",
      "Epoch 3/10\n",
      "277840/277840 [==============================] - 72s 260us/step - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "277840/277840 [==============================] - 73s 261us/step - loss: 0.6932 - acc: 0.5016 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "277840/277840 [==============================] - 73s 262us/step - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "277840/277840 [==============================] - 73s 264us/step - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 7/10\n",
      "277840/277840 [==============================] - 72s 258us/step - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "277840/277840 [==============================] - 72s 259us/step - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "277840/277840 [==============================] - 75s 269us/step - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "277840/277840 [==============================] - 73s 261us/step - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Test Loss:0.6932696543153929\n",
      "Test Accuracy:0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_count = 0\n",
    "\n",
    "for lstm_layer in lstm_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for dense_layer in dense_layers:\n",
    "            for batch_size in batch_sizes:\n",
    "                for dropout in dropouts:\n",
    "                    for optimizer in optimizers:\n",
    "                        for lr in learning_rates:\n",
    "                            for decay in decays:\n",
    "                                for loss_function in loss_functions:\n",
    "                                    model_name = 'BTC_VS_NoBTC'\n",
    "                                    model_name += '-exch-gemini'\n",
    "                                    model_name += '-predict-'+PREDICT_COLUMN \\\n",
    "                                                + '-lookback-'+str(LOOKBACK_HISTORY_SEQ_LEN) \\\n",
    "                                                + '-future-'+str(PREDICT_FUTURE_SEQ_LEN)\n",
    "                                    model_name += \"-lstm-{}-nodes-{}-dense-{}-batch-{}-dropout-{}-optimizer-{}-lr-{}-decay-{}\".format( \\\n",
    "                                                    lstm_layer, \\\n",
    "                                                    layer_size, \\\n",
    "                                                    dense_layer, \\\n",
    "                                                    batch_size, \\\n",
    "                                                    dropout, \\\n",
    "                                                    optimizer, \\\n",
    "                                                    lr, \\\n",
    "                                                    decay)\n",
    "\n",
    "                                    model_count += 1\n",
    "\n",
    "                                    print ('*Model {}/{}*'.format(model_count, number_of_models))\n",
    "                                    print (model_name)\n",
    "\n",
    "                                    # let user know if hyper parameters already trained\n",
    "                                    # this could happen if kernal crash and restarted training\n",
    "                                    check_logs_file = '{}/{}'.format(logs_to_folder, model_name)\n",
    "                                    if glob(check_logs_file+'*'):\n",
    "                                        print ('%s already exist.' % (check_logs_file))\n",
    "                                        # continue\n",
    "\n",
    "                                    # name of model by hyper-parameter values\n",
    "                                    # to compare them all in tensorboard\n",
    "                                    model_name += '-time-{}'.format(int(time.time()))\n",
    "\n",
    "                                    # LSTM layers - minimum of 1\n",
    "                                    model = Sequential()\n",
    "                                    model.add(CuDNNLSTM(layer_size, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "\n",
    "                                    # how much to forget\n",
    "                                    model.add(Dropout(dropout))\n",
    "\n",
    "                                    # normalize output, the output is the input for the next LSTM layer\n",
    "                                    model.add(BatchNormalization())\n",
    "\n",
    "                                    # LSTM layers - add more\n",
    "                                    for l in range(lstm_layer-1):\n",
    "                                        model.add(CuDNNLSTM(layer_size, return_sequences=True))\n",
    "                                        model.add(Dropout(dropout))\n",
    "                                        model.add(BatchNormalization())\n",
    "\n",
    "                                    model.add(CuDNNLSTM(layer_size))\n",
    "                                    model.add(Dropout(dropout))\n",
    "\n",
    "                                    # this LSTM layer do not need to return sequence, as the next layer is dense layer\n",
    "                                    model.add(BatchNormalization())\n",
    "\n",
    "                                    # Dense layers\n",
    "                                    for _ in range(dense_layer):\n",
    "                                        model.add(Dense(32, activation='relu'))\n",
    "                                        model.add(Dropout(dropout))\n",
    "\n",
    "                                    # the final layer only have 2 nodes for binary classification: Buy=1, Sell=0\n",
    "                                    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "                                    # optimizer\n",
    "                                    #   learning rate\n",
    "                                    #   learning decay\n",
    "                                    # opt = tf.keras.optimizers.Adam(lr=1e-3, decay=1e-6)\n",
    "                                    opt = optimizer(lr=lr, decay=decay)\n",
    "\n",
    "                                    # compile \n",
    "                                    model.compile(optimizer=opt, loss=loss_function, metrics=['accuracy'])\n",
    "\n",
    "                                    # save logs for tensorboard graphs\n",
    "                                    tensorboard = TensorBoard(log_dir=\"{}/{}\".format(logs_to_folder, model_name))\n",
    "\n",
    "                                    # saves only the best ones\n",
    "                                    filepath = \"RNN_-{epoch:02d}-{val_acc:.3f}\"\n",
    "                                    checkpoint = ModelCheckpoint('{}/{}.model'.format(models_to_folder, filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')) \n",
    "\n",
    "                                    # training\n",
    "                                    model.fit(X_train, y_train, epochs=EPOCHS, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=[tensorboard, checkpoint],)\n",
    "\n",
    "                                    # score\n",
    "                                    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "                                    # loss and accuracy\n",
    "                                    loss = score[0]\n",
    "                                    accuracy = score[1]\n",
    "                                    print('Test Loss:{}'.format(loss))\n",
    "                                    print('Test Accuracy:{}'.format(accuracy))\n",
    "\n",
    "                                    # save model\n",
    "                                    model.save('{}/{}'.format(models_to_folder, model_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Results\n",
    "\n",
    "The model validation accuracy is 0.5, meaning without the bitcoin data input, the model prediction is drastically reduced to benchmark level (random guess)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
